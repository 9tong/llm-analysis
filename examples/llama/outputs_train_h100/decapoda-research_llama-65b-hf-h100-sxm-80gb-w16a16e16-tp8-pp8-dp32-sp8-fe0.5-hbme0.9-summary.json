{
    "batch_size_per_gpu": 8,
    "max_batch_size_per_gpu": 9,
    "gradient_accumulation_steps": 15625,
    "global_batch_size": 4000000,
    "dp_size": 32,
    "tp_size": 8,
    "pp_size": 8,
    "sp_size": 8,
    "ds_zero": "NONE",
    "total_num_gpus": 2048,
    "seq_len": 2048,
    "total_num_tokens": 1400000000000.0,
    "num_params_total": 64686653440,
    "activation_recomputation": "SELECTIVE",
    "achived_flops": 989.5,
    "flops_efficiency": 0.5,
    "hbm_memory_efficiency": 0.9,
    "num_flops_total_per_micro_batch": 6710800500654080,
    "weight_memory_per_gpu": 2078801920.0,
    "gradient_memory_per_gpu": 2013265920.0,
    "optimizer_state_memory_per_gpu": 16106127360.0,
    "activation_memory_bs1": 6207569920.0,
    "activation_memory_per_gpu": 49660559360.0,
    "latency_per_micro_batch": 0.10596893160456797,
    "latency_fwd": 0.05835076843018432,
    "latency_fwd_attn": 0.012500763832723596,
    "latency_fwd_mlp": 0.022223580147064173,
    "latency_fwd_layernorm": 0.0004451665936981758,
    "latency_fwd_tp_comm": 0.020878313244444446,
    "latency_fwd_input_embedding": 0.0012178088628855722,
    "latency_fwd_output_embedding_loss": 0.0010851357493683678,
    "latency_per_iter": 1655.7645563213746,
    "total_training_latency": 281479.9745746337,
    "gpu_hours": 160130.82998023607
}
